{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c3daf4",
   "metadata": {},
   "source": [
    "### Instalaci칩n de librer칤as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9ff772a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%pip install gensim\n",
    "%pip install nltk\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052687f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0e8ac973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1b986175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to ./nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab', download_dir='./nltk_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df3768",
   "metadata": {},
   "source": [
    "### Preprocesamiento del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012471cf",
   "metadata": {},
   "source": [
    "Las siguientes funciones se encargan de preparar los textos de los siete libros de Harry Potter para el entrenamiento de modelos de embeddings con Word2Vec.\n",
    "\n",
    "El procesamiento de la funci칩n process_books_for_word2vec() sigue los siguientes pasos:\n",
    "\n",
    "1. Lectura de los libros: \n",
    "   Se leen todos los archivos .txt ubicados en la carpeta hp/. Cada archivo representa un libro completo.\n",
    "\n",
    "2. Segmentaci칩n en oraciones:  \n",
    "   Se utiliza la funci칩n sent_tokenize() de la librer칤a nltk para dividir cada texto en oraciones.\n",
    "\n",
    "3. Tokenizaci칩n de oraciones:  \n",
    "   Cada oraci칩n se tokeniza mediante simple_preprocess de Gensim, que:\n",
    "   - Convierte todas las palabras a min칰sculas,\n",
    "   - Convierte todas las palabras a min칰sculas,\n",
    "   - Elimina signos de puntuaci칩n,\n",
    "   - Elimina palabras con menos de 2 caracteres,\n",
    "   - Opcionalmente elimina acentos (deacc=True).\n",
    "\n",
    "El resultado final es una lista de listas de tokens (oraciones tokenizadas) que puede utilizarse directamente como entrada para entrenar modelos de embeddings con gensim.models.Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "783b52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_books(directory: Path) -> List[str]:\n",
    "    \"\"\"\n",
    "    Lee todos los archivos .txt dentro del directorio especificado.\n",
    "\n",
    "    :param directory: Ruta a la carpeta con los archivos de texto.\n",
    "    :return: Lista de strings, uno por cada libro completo.\n",
    "    \"\"\"\n",
    "    all_text = []\n",
    "    for book_file in sorted(directory.glob(\"*.txt\")):\n",
    "        with open(book_file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            all_text.append(text)\n",
    "    return all_text\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Divide un texto en oraciones usando la segmentaci칩n de NLTK.\n",
    "\n",
    "    :param text: Texto unificado.\n",
    "    :return: Lista de oraciones.\n",
    "    \"\"\"\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def tokenize_sentences(sentences: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tokeniza cada oraci칩n en palabras, eliminando puntuaci칩n y pasando a min칰sculas.\n",
    "\n",
    "    :param sentences: Lista de oraciones.\n",
    "    :return: Lista de listas de palabras (tokens).\n",
    "    \"\"\"\n",
    "    return [simple_preprocess(sentence, deacc=True) for sentence in sentences]\n",
    "\n",
    "def process_books_for_word2vec(directory: Path) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Procesa todos los libros en un directorio para generar oraciones tokenizadas\n",
    "    aptas para entrenamiento con Word2Vec.\n",
    "\n",
    "    :param directory: Ruta a la carpeta con los libros.\n",
    "    :return: Lista de oraciones tokenizadas.\n",
    "    \"\"\"\n",
    "    raw_books = read_all_books(directory)\n",
    "    all_sentences = []\n",
    "    for book in raw_books:\n",
    "        sentences = split_into_sentences(book)\n",
    "        tokenized = tokenize_sentences(sentences)\n",
    "        all_sentences.extend(tokenized)\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "433990b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游늯 Total de oraciones (docs): 66129\n"
     ]
    }
   ],
   "source": [
    "BOOKS_DIR = Path(\"hp\")\n",
    "\n",
    "sentences = process_books_for_word2vec(BOOKS_DIR)\n",
    "\n",
    "print(f\"游늯 Total de oraciones (docs): {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538ae83",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3804199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,        # Lista de oraciones tokenizadas\n",
    "    vector_size=100,            # Dimensi칩n de los vectores (puede ajustarse)\n",
    "    window=5,                   # Contexto de palabras (5 a la izquierda y 5 a la derecha)\n",
    "    min_count=5,                # Palabras con frecuencia < 5 son ignoradas\n",
    "    workers=4,                  # N칰mero de hilos de procesamiento paralelo\n",
    "    sg=1,                       # Skip-gram (1) en lugar de CBOW (0)\n",
    "    epochs=10                   # N칰mero de pasadas completas sobre el corpus\n",
    ")\n",
    "\n",
    "w2v_model.save(\"hp_word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e344f80",
   "metadata": {},
   "source": [
    "### Pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ceff0",
   "metadata": {},
   "source": [
    "#### Similitud sem치ntica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7875b98",
   "metadata": {},
   "source": [
    "Se realiza una exploraci칩n del espacio de embeddings entrenado con Word2Vec usando el m칠todo:\n",
    "\n",
    "```python\n",
    "w2v_model.wv.most_similar('palabra_objetivo')\n",
    "```\n",
    "\n",
    "Este devuelve las palabras que tienen mayor similitud coseno con el vector asociado a la palabra dada. Es decir, aquellas que aparecen en contextos similares y, por lo tanto, tienen un significado o funci칩n similar dentro del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "766831e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lord', 0.7069865465164185),\n",
       " ('wormtail', 0.6690964698791504),\n",
       " ('elder', 0.6340605616569519),\n",
       " ('nagini', 0.6284155249595642),\n",
       " ('pettigrew', 0.6125122904777527),\n",
       " ('understands', 0.611863911151886),\n",
       " ('weakness', 0.6112457513809204),\n",
       " ('dumbledore', 0.6090002655982971),\n",
       " ('quirrell', 0.6040014028549194),\n",
       " ('dumbledores', 0.6036220192909241)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('voldemort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d8f67bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ravenclaw', 0.72437983751297),\n",
       " ('slytherin', 0.7099413275718689),\n",
       " ('points', 0.698464572429657),\n",
       " ('hufflepuff', 0.6680050492286682),\n",
       " ('tower', 0.6656131148338318),\n",
       " ('spectators', 0.6427123546600342),\n",
       " ('locker', 0.6376153230667114),\n",
       " ('championship', 0.6364508867263794),\n",
       " ('chaser', 0.6340674161911011),\n",
       " ('captain', 0.6254349946975708)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('gryffindor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8eff5329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('match', 0.7520051002502441),\n",
       " ('season', 0.7355700135231018),\n",
       " ('session', 0.6944305896759033),\n",
       " ('team', 0.6925305128097534),\n",
       " ('seeker', 0.6807789206504822),\n",
       " ('cup', 0.6791298985481262),\n",
       " ('player', 0.6632496118545532),\n",
       " ('winning', 0.6535742878913879),\n",
       " ('practice', 0.6534658670425415),\n",
       " ('wins', 0.6499863862991333)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('quidditch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "521bbce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ron', 0.654772162437439),\n",
       " ('parvati', 0.6493538618087769),\n",
       " ('encouragingly', 0.6356828808784485),\n",
       " ('hotly', 0.6309463977813721),\n",
       " ('unconvinced', 0.6253873109817505),\n",
       " ('incredulously', 0.624884843826294),\n",
       " ('doubtfully', 0.6231666803359985),\n",
       " ('lavender', 0.6096546649932861),\n",
       " ('grumpily', 0.6076415181159973),\n",
       " ('fearfully', 0.6051589250564575)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('hermione')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa555c0",
   "metadata": {},
   "source": [
    "Se detectan palabras \"intrusas\" utilizando el m칠todo:\n",
    "\n",
    "```python\n",
    "w2v_model.wv.doesnt_match(lista_de_palabras)\n",
    "```\n",
    "\n",
    "Este sirve para detectar cu치l de las palabras no encaja sem치nticamente con el resto. Internamente, Word2Vec calcula el vector promedio de todas las palabras y devuelve aquella cuyo vector est치 m치s alejado del centroide de los dem치s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "07c1af2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'voldemort'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['harry', 'ron', 'hermione', 'voldemort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "93f4250b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weasley'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['malfoy', 'riddle', 'lestrange', 'weasley'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a7e38",
   "metadata": {},
   "source": [
    "#### Sem치ntica relacional mediante analog칤as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347c44c",
   "metadata": {},
   "source": [
    "Una de las capacidades m치s interesantes de los embeddings entrenados con Word2Vec es su habilidad para capturar relaciones sem치nticas vectoriales mediante operaciones aritm칠ticas simples. Se utiliza el m칠todo:\n",
    "\n",
    "```python\n",
    "w2v_model.wv.most_similar(positive=[...], negative=[...])\n",
    "```\n",
    "\n",
    "Este permite construir este tipo de analog칤as mediante sumas y restas de vectores. Internamente, calcula:\n",
    "\n",
    "$$\n",
    "\\text{resultado} = \\sum \\text{(positivos)} - \\sum \\text{(negativos)}\n",
    "$$\n",
    "\n",
    "y devuelve las palabras m치s similares al vector resultante, usando cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "26f53be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crabbe', 0.7475333213806152),\n",
       " ('pansy', 0.5595075488090515),\n",
       " ('parkinson', 0.52581387758255),\n",
       " ('guffawed', 0.504027247428894),\n",
       " ('cronies', 0.48564016819000244),\n",
       " ('george', 0.48499131202697754),\n",
       " ('zabini', 0.4760032296180725),\n",
       " ('sniggering', 0.4485675096511841),\n",
       " ('fred', 0.44503194093704224),\n",
       " ('gregory', 0.444950670003891)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"ron\", \"goyle\"], negative=[\"harry\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
