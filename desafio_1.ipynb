{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4de30f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (2.2.5)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0a6845",
   "metadata": {},
   "source": [
    "### Código inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "02c389c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import Bunch\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5668f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (11314, 101631)\n",
      "Cantidad de documentos: 11314\n",
      "Tamaño del vocabulario (dimensionalidad de los vectores): 101631\n",
      "F1 Score: 0.5854\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "tfidfvect = TfidfVectorizer()\n",
    "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
    "y_train = newsgroups_train.target\n",
    "\n",
    "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}\n",
    "idx = 4811\n",
    "cossim = cosine_similarity(X_train[idx], X_train)[0]\n",
    "np.sort(cossim)[::-1]\n",
    "np.argsort(cossim)[::-1]\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "X_test = tfidfvect.transform(newsgroups_test.data)\n",
    "y_test = newsgroups_test.target\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score_result = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'shape: {X_train.shape}')\n",
    "print(f'Cantidad de documentos: {X_train.shape[0]}')\n",
    "print(f'Tamaño del vocabulario (dimensionalidad de los vectores): {X_train.shape[1]}')\n",
    "print(f\"F1 Score: {f1_score_result:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580038cd",
   "metadata": {},
   "source": [
    "### Consigna del desafío 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc20569",
   "metadata": {},
   "source": [
    "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
    "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
    "la similaridad según el contenido del texto y la etiqueta de clasificación.\n",
    "\n",
    "**2**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
    "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
    "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
    "y ComplementNB.\n",
    "\n",
    "**3**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
    "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
    "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438cd903",
   "metadata": {},
   "source": [
    "#### Punto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4c89e471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Documento base (idx=10476) - Clase: rec.sport.hockey\n",
      "Contenido (primeros 300 caracteres): This is a general question for US readers:  How extensive is the playoff coverage down there?  In Canada, it is almost impossible not to watch a series on TV (ie the only two series I have not had an opportunity to watch this year are Wash-NYI and Chi-Stl, the latter because I'm in the wrong time zo\n",
      "\n",
      ">>> Similar #1: idx=5064 - Clase: rec.sport.hockey - Similitud coseno: 0.2250\n",
      "Contenido:  I only have one comment on this:  You call this a *classic* playoff year and yet you don't include a Chicago-Detroit series.  C'mon, I'm a Boston fan and I even realize that Chicago-Detroit games are THE most exciting games to watch.\n",
      "\n",
      ">>> Similar #2: idx=9623 - Clase: talk.politics.mideast - Similitud coseno: 0.2174\n",
      "Contenido: Accounts of Anti-Armenian Human Right Violations in Azerbaijan #012                  Prelude to Current Events in Nagorno-Karabakh          +---------------------------------------------------------+         |                                                         |         |  I saw a naked girl wi\n",
      "\n",
      ">>> Similar #3: idx=10575 - Clase: sci.crypt - Similitud coseno: 0.2164\n",
      "Contenido:  I am not an expert in the cryptography science, but some basic things seem evident to me, things which this Clinton Clipper do not address. The all pertain to opportunites for abuse, and conclusions based on what I have seen the membership of this group (except for two notable persons) agree to.  I\n",
      "\n",
      ">>> Similar #4: idx=10836 - Clase: alt.atheism - Similitud coseno: 0.2126\n",
      "Contenido: Archive-name: atheism/faq Alt-atheism-archive-name: faq Last-modified: 5 April 1993 Version: 1.1                      Alt.Atheism Frequently-Asked Questions  This file contains responses to articles which occur repeatedly in alt.atheism.  Points covered here are ones which are not covered in the \"In\n",
      "\n",
      ">>> Similar #5: idx=2350 - Clase: sci.crypt - Similitud coseno: 0.2111\n",
      "Contenido: Archive-name: net-privacy/part1 Last-modified: 1993/3/3 Version: 2.1   IDENTITY, PRIVACY, and ANONYMITY on the INTERNET ================================================  (c) 1993 L. Detweiler.  Not for commercial use except by permission from author, otherwise may be freely copied.  Not to be altere\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Documento base (idx=1824) - Clase: comp.sys.mac.hardware\n",
      "Contenido (primeros 300 caracteres):   \tI think this kind of comparison is pretty useless in general.  The processor is only good when a good computer is designed around it adn the computer is used in its designed purpose.  Comparing processor speed is pretty dumb because all you have to do is just increase the clock speed to increase \n",
      "\n",
      ">>> Similar #1: idx=9921 - Clase: comp.sys.mac.hardware - Similitud coseno: 0.3542\n",
      "Contenido: dhk@ubbpc.uucp (Dave Kitabjian) writes ...  040 486 030 386 020 286   060 fastest, then Pentium, with the first versions of the PowerPC somewhere in the vicinity.   No.  Computer speed is only partly dependent of processor/clock speed. Memory system speed play a large role as does video system speed\n",
      "\n",
      ">>> Similar #2: idx=6364 - Clase: comp.sys.mac.hardware - Similitud coseno: 0.3132\n",
      "Contenido: Well folks, after some thought the answer struck me flat in the face:  \"Why would Apple release a Duo Dock with a processor of its own?\"  Here's why- People have hounded Apple for a notebook with a 68040 processor in it. Apple can't deliver that right now because the 040 saps too much power, radiate\n",
      "\n",
      ">>> Similar #3: idx=5509 - Clase: comp.sys.mac.hardware - Similitud coseno: 0.3041\n",
      "Contenido: rvenkate@ux4.cso.uiuc.edu (Ravikuma Venkateswar) writes ...  Benchmarks are for marketing dweebs and CPU envy.  OK, if it will make you happy, the 486 is faster than the 040.  BFD.  Both architectures are nearing then end of their lifetimes.  And especially with the x86 architecture: good riddance. \n",
      "\n",
      ">>> Similar #4: idx=2641 - Clase: comp.sys.mac.hardware - Similitud coseno: 0.2504\n",
      "Contenido:      I think this is mostly the fault of the people who write up the literature and price lists being confused themselves. Since there are two possible processor configurations and one of the them doesn't have an FPU it does seem to be an option, even though it really isn't.       Well, then allow m\n",
      "\n",
      ">>> Similar #5: idx=4359 - Clase: comp.sys.mac.hardware - Similitud coseno: 0.2417\n",
      "Contenido: If you get the Centris 650 with CD configuration, you are getting a Mac with a 68RC040 processor that has built-in math coprocessor support.  My  understanding is that the \"optional fpu\" refers to your option of purchasing the Centris 650 4/80 without FPU OR one of the other configurations WITH FPU.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Documento base (idx=409) - Clase: comp.graphics\n",
      "Contenido (primeros 300 caracteres): I can't fiqure this out.  I have properly compiled pov on a unix machine running SunOS 4.1.3  The problem is that when I run the sample .pov files and use the EXACT same parameters when compiling different .tga outputs.  Some of the .tga's are okay, and other's are unrecognizable by any software.\n",
      "\n",
      ">>> Similar #1: idx=3444 - Clase: comp.graphics - Similitud coseno: 0.2305\n",
      "Contenido: Hi, I'm just getting into PoVRay and I was wondering if there is a graphic package that outputs .POV files.  Any help would be appreciated. Thanks.  Later'ish Craig \n",
      "\n",
      ">>> Similar #2: idx=5799 - Clase: comp.graphics - Similitud coseno: 0.2091\n",
      "Contenido: I finally got a 24 bit viewer for my POVRAY generated .TGA files.  It was written in C by Sean Malloy and he kindly sent me a copy.  He wrote it for the same purpose, to view .TGA files using his SpeedStar 24.  It ONLY works with the SpeedStar 24 and I cannot send copies since it is not my program. \n",
      "\n",
      ">>> Similar #3: idx=5905 - Clase: comp.graphics - Similitud coseno: 0.1982\n",
      "Contenido:  Hallo POV-Renderers ! I've got a BocaX3 Card. Now I try to get POV displaying True Colors while rendering. I've tried most of the options and UNIVESA-Driver but what happens isn't correct. Can anybody help me ? \n",
      "\n",
      ">>> Similar #4: idx=1764 - Clase: comp.graphics - Similitud coseno: 0.1838\n",
      "Contenido: hi guys  like all people in this group i'm a fans of fractal and render sw  my favourite are fractint pov & 3dstudio 2.0   now listen my ideas  i'have just starting now to be able to use 3dstudio quite well  so i'm simulating a full animation of a f1 grand prix  unfortanatly just some lap(10?)  i' m\n",
      "\n",
      ">>> Similar #5: idx=3364 - Clase: comp.graphics - Similitud coseno: 0.1659\n",
      "Contenido: Does anyone know of a good way (standard PC application/PD utility) to convert tif/img/tga files into LaserJet III format.  We would also like to do the same, converting to HPGL (HP plotter) files.  Please email any response.  Is this the correct group?  Thanks in advance.  Michael.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Documento base (idx=4506) - Clase: rec.autos\n",
      "Contenido (primeros 300 caracteres):  This does sound good, but I heard it tends to leave more grit, etc in the  oil pan.  Also, I've been told to change the old when it's hot before the grit has much time to settle.  Any opinions? \n",
      "\n",
      ">>> Similar #1: idx=4211 - Clase: rec.motorcycles - Similitud coseno: 0.1894\n",
      "Contenido:   It's normal for the BMW K bikes to use a little oil in the first few thousand  miles.  I don't know why.  I've had three new K bikes, and all three used a bit of oil when new - max maybe .4 quart in first 1000 miles; this soon quits and by the time I had 10,000 miles on them the oil consumption wa\n",
      "\n",
      ">>> Similar #2: idx=5928 - Clase: comp.sys.mac.hardware - Similitud coseno: 0.1682\n",
      "Contenido: or there   Okay, I guess its time for a quick explanation of Mac sound.  The original documentation for the sound hardware (IM-3) documents how to make sound by directly accessing hardware.  Basically, you jam values into all the even bytes from SoundBase to SoundBase+0x170. This was because of how \n",
      "\n",
      ">>> Similar #3: idx=6224 - Clase: rec.autos - Similitud coseno: 0.1583\n",
      "Contenido: Archive-name: rec-autos/part5  [this article is one of a pair of articles containing commonly asked automotive questions; the other article contains questions more geared to the automotive enthusiast, and so is not crossposted to misc.consumers.  -- rpw]     [changes as of 14 April 1993: revised bra\n",
      "\n",
      ">>> Similar #4: idx=5171 - Clase: rec.autos - Similitud coseno: 0.1577\n",
      "Contenido:  If the tire has a leak you should fix it.    Doesn't work too well if the engine is hot, its more accurate to check the oil when the engine is cool, i.e. not when you are at a gas station.\n",
      "\n",
      ">>> Similar #5: idx=9491 - Clase: rec.autos - Similitud coseno: 0.1522\n",
      "Contenido: My friend brought a subaru SVX recently.  I had drove it for couples times and I think its a great car, esp on snow.  However when she took it to a local Subaru dealer for a oil change, the bill came out to be about 80 dollars.  The dealer told us it is because to change the oil filter on a SVX it i\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> Documento base (idx=4012) - Clase: rec.sport.hockey\n",
      "Contenido (primeros 300 caracteres): For those Leaf fans who are concerned, the following players are slated for return on Thursday's Winnipeg-Toronto game :     Peter Zezel, John Cullen    Mark Osborne and Dave Ellett are questionable to return on Thursday.\n",
      "\n",
      ">>> Similar #1: idx=6599 - Clase: soc.religion.christian - Similitud coseno: 0.1600\n",
      "Contenido: True.  Also read 2 Peter 3:16  Peter warns that the scriptures are often hard to understand by those who are not learned on the subject.\n",
      "\n",
      ">>> Similar #2: idx=10644 - Clase: rec.sport.hockey - Similitud coseno: 0.1428\n",
      "Contenido: In  <1qvos8$r78@cl.msu.>, vergolin@euler.lbs.msu.edu (David Vergolini) writes...  There's quite a few Wings fans lurking about here, they just tend to be low key and thoughtful rather than woofers.  I suppose every family must have a Roger Clinton, though.  But remember (to paraphrase one of my favo\n",
      "\n",
      ">>> Similar #3: idx=7478 - Clase: rec.sport.hockey - Similitud coseno: 0.1358\n",
      "Contenido: Toronto                          1 1 1--3 Detroit                          1 4 1--6 First period      1, Detroit, Yzerman 1 (Gallant, Ciccarelli) 4:48.      2, Toronto, Cullen 1 (Clark, Gill) 10:44. Second period      3, Detroit, Sheppard 1 (Probert, Coffey) pp, 5:04.      4, Detroit, Burr 1 (Racine\n",
      "\n",
      ">>> Similar #4: idx=7308 - Clase: rec.sport.hockey - Similitud coseno: 0.1318\n",
      "Contenido: Detroit is a very disciplined team.  There's a lot of Europeans in Detroit which would make the game fast, so Toronto would have to slow the game down, which means drawing penalties, as a last resort anyway.  Toronto will be a good team as soon as they get more good players.  Toronto is just an aver\n",
      "\n",
      ">>> Similar #5: idx=10792 - Clase: rec.sport.baseball - Similitud coseno: 0.1307\n",
      "Contenido:   The tribe will be in town from April 16 to the 19th. There are ALWAYS tickets available! (Though they are playing Toronto, and many Toronto fans make the trip to Cleveland as it is easier to get tickets in Cleveland than in Toronto.  Either way, I seriously doubt they will sell out until the end o\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_random_doc_indices(n: int, total_docs: int, seed: Optional[int] = None) -> List[int]:\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    return random.sample(range(total_docs), n)\n",
    "\n",
    "\n",
    "def get_most_similar_docs(X: csr_matrix, idx: int, top_n: int = 5) -> List[Dict[str, Any]]:\n",
    "    similarities = cosine_similarity(X[idx], X)[0]\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    most_similar = []\n",
    "    for i in sorted_indices[1:top_n+1]:\n",
    "        most_similar.append({'doc_index': i, 'cos_sim': float(similarities[i])})\n",
    "    return most_similar\n",
    "\n",
    "\n",
    "def print_similar_docs_info(idx: int, similar_docs: List[Dict[str, Any]], dataset: Bunch) -> None:\n",
    "    base_class = dataset.target_names[dataset.target[idx]]\n",
    "    base_text = dataset.data[idx][:300].replace('\\n', ' ')\n",
    "    print(f\"\\n>>> Documento base (idx={idx}) - Clase: {base_class}\")\n",
    "    print(f\"Contenido (primeros 300 caracteres): {base_text}\\n\")\n",
    "\n",
    "    for i, doc in enumerate(similar_docs, start=1):\n",
    "        doc_idx = doc['doc_index']\n",
    "        cos_sim = doc['cos_sim']\n",
    "        doc_class = dataset.target_names[dataset.target[doc_idx]]\n",
    "        doc_text = dataset.data[doc_idx][:300].replace('\\n', ' ')\n",
    "        print(f\">>> Similar #{i}: idx={doc_idx} - Clase: {doc_class} - Similitud coseno: {cos_sim:.4f}\")\n",
    "        print(f\"Contenido: {doc_text}\\n\")\n",
    "\n",
    "\n",
    "random_indices = get_random_doc_indices(n=5, total_docs=X_train.shape[0], seed=42)\n",
    "\n",
    "for idx in random_indices:\n",
    "    similar_docs = get_most_similar_docs(X_train, idx, top_n=5)\n",
    "    print_similar_docs_info(idx, similar_docs, newsgroups_train)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f42a27",
   "metadata": {},
   "source": [
    "Los resultados muestran que la similitud coseno entre vectores TF-IDF permite recuperar documentos con contenido relacionado al documento tomado de ejemplo, tanto en términos semánticos como de categoría. En los casos en los que la similitud es relativamente alta, los documentos más cercanos suelen pertenecer a la misma clase y comparten vocabulario específico del tema, lo que valida la efectividad de la representación vectorial. \n",
    "\n",
    "Sin embargo, cuando la similitud es baja, los documentos recuperados frecuentemente no tienen relación semántica clara ni coinciden en la categoría, lo que indica que en esos casos el vectorizador no logra capturar adecuadamente el significado del texto. Por lo tanto, la utilidad de esta métrica está fuertemente condicionada por la calidad de la vectorización y por el umbral de similitud alcanzado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900f3a4",
   "metadata": {},
   "source": [
    "#### Punto 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f31b5a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score macro (MultinomialNB): 0.6425\n",
      "F1-score macro (ComplementNB): 0.7039\n"
     ]
    }
   ],
   "source": [
    "def vectorize_data(train_data: list, test_data: list) -> Tuple[TfidfVectorizer, csr_matrix, csr_matrix]:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "    )\n",
    "    X_train = vectorizer.fit_transform(train_data)\n",
    "    X_test = vectorizer.transform(test_data)\n",
    "    return vectorizer, X_train, X_test\n",
    "\n",
    "\n",
    "def train_classifier(model: ClassifierMixin, X_train: csr_matrix, y_train: list) -> ClassifierMixin:\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model: ClassifierMixin, X_test: csr_matrix, y_test: list) -> float:\n",
    "    y_pred = model.predict(X_test)\n",
    "    return f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "\n",
    "vectorizer, X_train_vec, X_test_vec = vectorize_data(newsgroups_train.data, newsgroups_test.data)\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "multinomial_model = train_classifier(MultinomialNB(), X_train_vec, y_train)\n",
    "f1_multinomial = evaluate_model(multinomial_model, X_test_vec, y_test)\n",
    "print(f\"F1-score macro (MultinomialNB): {f1_multinomial:.4f}\")\n",
    "\n",
    "complement_model = train_classifier(ComplementNB(), X_train_vec, y_train)\n",
    "f1_complement = evaluate_model(complement_model, X_test_vec, y_test)\n",
    "print(f\"F1-score macro (ComplementNB): {f1_complement:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f8c05",
   "metadata": {},
   "source": [
    "Luego de evaluar distintas configuraciones del vectorizador TfidfVectorizer, se concluye que la eliminación de stop words en inglés es la que más impacto tiene en el desempeño del modelo, mejorando el F1-score macro de 0.5854 a 0.6425.\n",
    "\n",
    "Adicionalmente, la incorporación de bigramas mediante ngram_range=(1, 2) proporciona una mejora adicional, alcanzando un F1-score macro de 0.7039 con el clasificador ComplementNB.\n",
    "\n",
    "Otras modificaciones como sublinear_tf, min_df o max_df no mejoraron el rendimiento en este caso y, en algunos casos, lo empeoraron. Por lo tanto, la mejor configuración se logra combinando eliminación de stopwords con uso de unigramas y bigramas, manteniendo el resto de los parámetros por defecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d42b9d",
   "metadata": {},
   "source": [
    "#### Punto 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8c3d71b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras similares a 'hockey':\n",
      "  - hockey league (Similitud: 0.3003)\n",
      "  - ncaa (Similitud: 0.2988)\n",
      "  - nhl (Similitud: 0.2938)\n",
      "  - hockey players (Similitud: 0.2911)\n",
      "  - market hockey (Similitud: 0.2884)\n",
      "--------------------------------------------------------------------------------\n",
      "Palabras similares a 'god':\n",
      "  - god god (Similitud: 0.3086)\n",
      "  - god does (Similitud: 0.3002)\n",
      "  - bible (Similitud: 0.2995)\n",
      "  - jesus (Similitud: 0.2977)\n",
      "  - christ (Similitud: 0.2962)\n",
      "--------------------------------------------------------------------------------\n",
      "Palabras similares a 'car':\n",
      "  - car car (Similitud: 0.2792)\n",
      "  - new car (Similitud: 0.2653)\n",
      "  - bought car (Similitud: 0.2602)\n",
      "  - car dealer (Similitud: 0.2212)\n",
      "  - car like (Similitud: 0.2122)\n",
      "--------------------------------------------------------------------------------\n",
      "Palabras similares a 'computer':\n",
      "  - computer science (Similitud: 0.2355)\n",
      "  - computer graphics (Similitud: 0.2342)\n",
      "  - turn computer (Similitud: 0.2062)\n",
      "  - cuz everytime (Similitud: 0.1783)\n",
      "  - damaging chips (Similitud: 0.1783)\n",
      "--------------------------------------------------------------------------------\n",
      "Palabras similares a 'sports':\n",
      "  - sports radio (Similitud: 0.4440)\n",
      "  - jody macdonald (Similitud: 0.4291)\n",
      "  - steve fredericks (Similitud: 0.4150)\n",
      "  - best sports (Similitud: 0.4068)\n",
      "  - wip (Similitud: 0.4066)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_idx2word_mapping(vectorizer: TfidfVectorizer) -> Dict[int, str]:\n",
    "    return {v: k for k,v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "\n",
    "def get_most_similar_words(\n",
    "        X_T: csr_matrix, vectorizer: TfidfVectorizer, word: str, top_n: int = 5\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    if word not in vectorizer.vocabulary_:\n",
    "        raise ValueError(f\"La palabra '{word}' no está en el vocabulario del vectorizador.\")    \n",
    "    word_idx = vectorizer.vocabulary_[word]\n",
    "    similarities = cosine_similarity(X_T[word_idx], X_T)[0]\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    idx2word = get_idx2word_mapping(vectorizer)\n",
    "    most_similar = []\n",
    "    for idx in sorted_indices[1:top_n+1]:\n",
    "        similar_word = idx2word[idx]\n",
    "        most_similar.append({'word': similar_word, 'cos_sim': float(similarities[idx])})    \n",
    "    return most_similar\n",
    "\n",
    "\n",
    "X_train_T = X_train_vec.transpose()\n",
    "words_to_test = ['hockey', 'god', 'car', 'computer', 'sports',]\n",
    "for word in words_to_test:\n",
    "    try:\n",
    "        similar_words = get_most_similar_words(X_train_T, vectorizer=vectorizer, word=word, top_n=5)\n",
    "        print(f\"Palabras similares a '{word}':\")\n",
    "        for similar_word in similar_words:\n",
    "            print(f\"  - {similar_word['word']} (Similitud: {similar_word['cos_sim']:.4f})\")\n",
    "        print(\"-\" * 80)\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d0128",
   "metadata": {},
   "source": [
    "Como se vio en clase, al transponer la matriz documento-término se obtiene una matriz término-documento, donde cada fila representa el patrón de ocurrencia de una palabra a lo largo de todos los documentos del corpus. Esta representación permite aplicar la similitud coseno entre palabras, interpretándolas como vectores de contexto.\n",
    "\n",
    "Los resultados obtenidos muestran que esta aproximación capta posibles relaciones semánticas. Por ejemplo, la palabra \"god\" se asocia fuertemente con \"jesus\", \"bible\" y \"christ\", lo que refleja que estos términos aparecen frecuentemente en documentos similares, reforzando la idea de que comparten contexto semántico. De forma análoga, \"hockey\" se vincula con \"nhl\" (National Hockey League), \"hockey league\" y \"ncaa\" (National Collegiate Athletic Association), todos términos específicos del dominio deportivo.\n",
    "\n",
    "Esto sugiere que la vectorización TF-IDF no solo permite representar documentos, sino que también habilita el análisis de relaciones entre palabras a partir de su distribución en el corpus. Aunque esta técnica no capta significado profundo como lo haría un modelo de embeddings, es suficiente para identificar asociaciones contextuales claras dentro del dominio del dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
